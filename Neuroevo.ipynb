{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gym\n",
    "import cv2\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from rq import Queue\n",
    "from redis import Redis\n",
    "\n",
    "import imageio\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "module_path = os.path.abspath(os.path.join('.'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from ga_model import *\n",
    "\n",
    "def to_np(x):\n",
    "    return x.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALsAAADuCAYAAABh03YuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAA99JREFUeJzt3DFuFGcYgOF/CHXEAazQUOQIiANYLnIanyAn8AFyAMQBKCyKlBHXoEdIWCgFhScFRaSVEDv4tz32+zytZz9P8erb2fF4l3VdBxQ8ue8TgLsidjLETobYyRA7GWInQ+xkiJ0MsZPxdMvBy7L4cyu7s67rcsxxNjsZYidD7GRsumbfo4uLi82vOT8/v9GMw9fPmnFTeziHQ4fndBe/83tsdjIe/GY/dBtb9z7ePWa4zy26RzY7GY9us/O/H72b1Da/zU6Gzf6I/GhT38fnhj2x2cl4dJt9xvbay4yH8DsfEpudDLGTsWz5kiSP+LJHHvGFA5s+oJ6cnOT+EMG+bflQbrOTIXYyxE6G2MkQOxliJ0PsZIidDLGTIXYyxE6G2MkQOxliJ2Pq/6B6/JfbMOt/a212MsROhtjJEDsZYidD7GSInQyxkyF2MsROhtjJEDsZYidD7GSInQyxkyF2MsROhtjJEDsZYidD7GSInQyxkyF2MqZ+I9j7s7OZ42CMMcY/k+bY7GSInQyxkyF2MsROxtS7MdcvrmaOg6lsdjLETobYyRA7GWInQ+xkTL31+OnXf2eOg6lsdjLETobYyRA7GWInY+7dmN+/zhwH33ycM8ZmJ0PsZIidDLGTIXYypt6NeX3928xxMMYY43TSHJudDLGTIXYyxE6G2MmYejfm65s/Z46Db07nfGm1zU6G2MkQOxliJ0PsZIidjKm3Hv++fDlzHIwxxvjj9GLKHJudDLGTIXYyxE6G2MkQOxliJ0PsZIidDLGTIXYyxE6G2MkQOxliJ0PsZIidDLGTIXYyxE6G2MkQOxliJ0PsZIidDLGTIXYyxE6G2MkQOxliJ0PsZIidDLGTIXYyxE6G2MkQOxliJ0PsZIidDLGTIXYyxE6G2MkQOxliJ0PsZIidDLGTIXYyxE6G2MkQOxliJ0PsZIidDLGTIXYyxE6G2MkQOxliJ0PsZIidDLGTIXYyxE6G2MkQOxliJ0PsZIidDLGTIXYyxE6G2MkQOxliJ0PsZIidDLGTIXYyxE6G2MkQOxliJ0PsZIidDLGTIXYyxE6G2MkQOxliJ0PsZIidDLGT8XTLwZ9/uR5vn325rXPhBt6fnd3o9S8vLyedyXyv3r377s/+uro6eo7NTobYyRA7GZuu2dmvPV9z74XNTobNzu7Netda1nU9/uBlOf5guCPrui7HHOcyhgyxkyF2MsROhtjJEDsZYidD7GSInQyxkyF2MsROhtjJEDsZW59n/zjG+HAbJwI/6fmxB256nh0eMpcxZIidDLGTIXYyxE6G2MkQOxliJ0PsZPwH52ZneyBJojYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO4AAADuCAYAAAA+7jsiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAABRNJREFUeJzt3bFqVGkYgOGZzRaCIJYiSLDxAqxDChux8CZsrOwEb8PWmxDBQmwsxFrBFDYWBsTGrQRjYfBsE4bJrElm3DnGV5+n+mV+v5yRvPxnkkmcDsMwAVr+Ou0LAFYnXAgSLgQJF4KEC0HChSDhQpBwIUi4EPT3Kpun06m3WcHIhmGYnrTHiQtBK524dTdv3pytL168+EMzHjx4cORjt2/fXnnehw8fDv358ePH/2veouOud91q17vouM+P07yu73HiQpBwIUi4EDRd5edx619VfvLkyWy9vb09W589e/bQvq2trdn66dOnhx5b3Dvv8+fPs/X169dn6xcvXhy57/nz54ceu3Hjxnf3LZq/jvnnNZkc/9zGtHi9889t/nn9qo76/JhMfu6/o68qw29KuBD0R307iHGdO3fuyMc2NjZm60+fPh2572fekpY5cSFIuBAkXAj6o17jvnr1arbe39+freffZrjo2bNnS8+f33vv3r3vrhf37ezs/NDHnr/m+edy0t8b06NHj5bad1rXd5KjPj9+RU5cCBIuBK30zqmrV68Oi+8CAtZna2tr8vLlS++cgt+RcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFoFF+55TfjQv/ddx/KbMqJy4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQNMrvVX748OEYY4EDTlwIEi4EjXKrfP78+THGAgecuBAkXAgSLgSN8hr38uXLY4wFDjhxIUi4ECRcCBIuBAkXgoQLQaN8O2hjY2OMscABJy4ECReChAtBwoUg4ULQKF9VvnTp0hhjIW1vb29ts5y4ECRcCBIuBAkXgoQLQcKFoFG+HfTmzZsxxkLa5ubm2mY5cSFIuBAkXAga5TXu7u7uGGMhzWtc+MMJF4JGuVV++/btGGMhbXt7e22znLgQJFwIGuVW+c6dO2OMhbRbt26tbZYTF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0F/r7J5GIbJ169fx7oWgs6cOTNbX7hwYe3z3717t/aZp2Vvb+/EPd++fVtqlhMXgoQLQcKFoJVe4+7v708+fvw41rUQdOXKldn6/v37a59/7dq1tc88LTs7Oyfu+fLly1KznLgQJFwIWulWGRa9fv16tv6dbmvHcPfu3RP3vH//fqlZTlwIEi4ETYdhWH7zdLr8ZuCHDMMwPWmPExeChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIWjVH6T/ZzKZ7I5xIcBkMplMNpfZtNKP9QG/BrfKECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQ9C+brrsfnkxhzQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_img(im, figsize=None, ax=None):\n",
    "    if not ax: fig,ax = plt.subplots(figsize=figsize)\n",
    "    if len(im.shape) == 2:\n",
    "        ax.imshow(im, cmap='gray')\n",
    "    else:\n",
    "        ax.imshow(im)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    return ax\n",
    "\n",
    "def render(env, converted=False):\n",
    "    state = env.render(mode='rgb_array')\n",
    "    if converted:\n",
    "        state = convert_state(state)\n",
    "    plt.show(show_img(state))\n",
    "    \n",
    "reset(env)\n",
    "print(env.action_space)\n",
    "render(env)\n",
    "render(env, converted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We use the larger DQN architecture from Mnih et al. (2015) consisting of 3 convolutional layers with 32, 64, and 64 channels followed by a hidden layer\n",
    "with 512 units. The convolutional layers use 8 × 8, 4 × 4, and 3 × 3 filters with strides of 4, 2, and 1, respectively. All hidden layers were followed by a rectifier nonlinearity (ReLU). The network contains over 4M parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []\n",
    "for _ in range(4):\n",
    "    states.append(step(env, env.action_space.sample())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first try\n",
      "[[-0.09741184  0.26149702 -0.3890749   0.67229307 -0.5611946  -0.6819385\n",
      "   0.7464271   0.03711712  0.92939895 -1.0250591  -0.7019196   0.14549139\n",
      "   0.49412838 -0.26982388 -0.30226147  0.71144384  0.11021616  0.04468439]]\n",
      "didn't change anything, should be the same\n",
      "[[-0.09741184  0.26149702 -0.3890749   0.67229307 -0.5611946  -0.6819385\n",
      "   0.7464271   0.03711712  0.92939895 -1.0250591  -0.7019196   0.14549139\n",
      "   0.49412838 -0.26982388 -0.30226147  0.71144384  0.11021616  0.04468439]]\n",
      "after evolution, should be different but only slightly\n",
      "[[-0.12135802  0.30858874 -0.37227494  0.6780739  -0.48251632 -0.54045635\n",
      "   0.81291544 -0.09002674  0.9011262  -1.0612363  -0.76098007  0.26238817\n",
      "   0.44562003 -0.39223626 -0.31144577  0.748786    0.05704778 -0.03440523]]\n",
      "copied model, should be the same\n",
      "[[-0.12135802  0.30858874 -0.37227494  0.6780739  -0.48251632 -0.54045635\n",
      "   0.81291544 -0.09002674  0.9011262  -1.0612363  -0.76098007  0.26238817\n",
      "   0.44562003 -0.39223626 -0.31144577  0.748786    0.05704778 -0.03440523]]\n"
     ]
    }
   ],
   "source": [
    "m = Model(random_state())\n",
    "print('first try')\n",
    "print(to_np(m(Variable(torch.Tensor([states])))))\n",
    "print('didn\\'t change anything, should be the same')\n",
    "print(to_np(m(Variable(torch.Tensor([states])))))\n",
    "m.evolve(0.005, random_state())\n",
    "print('after evolution, should be different but only slightly')\n",
    "print(to_np(m(Variable(torch.Tensor([states])))))\n",
    "\n",
    "m2 = uncompress_model(m.compress())\n",
    "print('copied model, should be the same')\n",
    "print(to_np(m2(Variable(torch.Tensor([states])))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok great, it compiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "REDIS_HOST = '172.31.7.58'\n",
    "\n",
    "class FakeJob:\n",
    "    def __init__(self, j):\n",
    "        self.result = j.result\n",
    "\n",
    "class GA:\n",
    "    def __init__(self, population, compressed_models=None, queue_name='default'):\n",
    "        self.population = population\n",
    "        self.models = [CompressedModel() for _ in range(population)] if compressed_models is None else compressed_models\n",
    "        \n",
    "        self.redis = Redis(REDIS_HOST)\n",
    "        self.queue = Queue(connection=self.redis, name=queue_name)\n",
    "        for j in self.queue.jobs:\n",
    "            j.cancel()\n",
    "\n",
    "    # Note: the paper says \"20k frames\", but there are 4 frames per network\n",
    "    # evaluation, so we cap at 5k evaluations\n",
    "    def get_best_models(self, env, max_eval=5000, max_noop=30):\n",
    "        jobs = []\n",
    "        for m in self.models:\n",
    "            jobs.append(self.queue.enqueue(evaluate_model, env, m, max_eval=max_eval, max_noop=max_noop, ttl=650, timeout=600))\n",
    "        last_enqueue_time = time.time()\n",
    "        while True:\n",
    "            for i in range(len(jobs)):\n",
    "                if jobs[i].result is not None and not isinstance(jobs[i], FakeJob):\n",
    "                    if random.random() < 0.001:\n",
    "                        print(jobs[i].result)\n",
    "                    jobs[i] = FakeJob(jobs[i])\n",
    "            scores = [j.result[0] if j.result is not None else j.result for j in jobs]\n",
    "            if None not in scores:\n",
    "                break\n",
    "            if time.time() - last_enqueue_time > 60:\n",
    "                print(f'Reenqueuing unfinished jobs ({sum(x is None for x in scores)}).')\n",
    "                for i in range(len(jobs)):\n",
    "                    if jobs[i].result is None:\n",
    "                        jobs[i].cancel()\n",
    "                        jobs[i] = self.queue.enqueue(\n",
    "                            evaluate_model, env, self.models[i], max_eval=max_eval, max_noop=max_noop, ttl=650, timeout=600)\n",
    "                last_enqueue_time = time.time()\n",
    "            time.sleep(1)\n",
    "        used_frames = sum(j.result[1] for j in jobs)\n",
    "        scored_models = list(zip(self.models, scores))\n",
    "        scored_models.sort(key=lambda x: x[1], reverse=True)\n",
    "        return scored_models, used_frames\n",
    "\n",
    "    def evolve_iter(self, env, sigma=0.005, truncation=10, max_eval=5000, max_noop=30):\n",
    "        scored_models, used_frames = self.get_best_models(env, max_eval=max_eval, max_noop=max_noop)\n",
    "        scores = [s for _, s in scored_models]\n",
    "        median_score = np.median(scores)\n",
    "        mean_score = np.mean(scores)\n",
    "        max_score = scored_models[0][1]\n",
    "        scored_models = scored_models[:truncation]\n",
    "        \n",
    "        # Elitism\n",
    "        self.models = [scored_models[0][0]]\n",
    "        for _ in range(self.population):\n",
    "            self.models.append(copy.deepcopy(random.choice(scored_models)[0]))\n",
    "            self.models[-1].evolve(sigma)\n",
    "            \n",
    "        return median_score, mean_score, max_score, used_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga = GA(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_frames = 0\n",
    "while total_frames < 1_000_000_000:\n",
    "    med, avg, M, frames = ga.evolve_iter(env.spec.id)\n",
    "    total_frames += frames\n",
    "    print(f'Done with generation!\\nMedian: {med}, average: {avg}, max: {M}, frames: {total_frames:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = ga.get_best_models(env.spec.id)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(best_models, open(f'{env.spec.id}_best.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = pickle.load(open(f'{env.spec.id}_best.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_video(env, model, max_eval=200000, max_noop=30):\n",
    "    env = gym.make(env)\n",
    "    model = uncompress_model(model)\n",
    "    noops = random.randint(0, max_noop)\n",
    "    cur_states = [reset(env)] * 4\n",
    "    total_reward = 0\n",
    "    frames = [env.render(mode='rgb_array')]\n",
    "    for _ in range(noops):\n",
    "        cur_states.pop(0)\n",
    "        new_state, reward, is_done, _ = step(env, 0)\n",
    "        frames.append(env.render(mode='rgb_array'))\n",
    "        total_reward += reward\n",
    "        if is_done:\n",
    "            return total_reward\n",
    "        cur_states.append(new_state)\n",
    "\n",
    "    model.eval()\n",
    "    for _ in range(max_eval):\n",
    "        values = model(Variable(torch.Tensor([cur_states])))[0]\n",
    "        action = np.argmax(values.data.numpy())\n",
    "        new_state, reward, is_done, _ = step(env, action)\n",
    "        frames.append(env.render(mode='rgb_array'))\n",
    "        total_reward += reward\n",
    "        if is_done:\n",
    "            break\n",
    "        cur_states.pop(0)\n",
    "        cur_states.append(new_state)\n",
    "\n",
    "    return total_reward, frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, video = make_video(env.spec.id, best_models[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score)\n",
    "imageio.mimsave(f'{env.spec.id}.gif', video + [video[-1]] * 10, fps=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
